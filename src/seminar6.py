{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM47X0+U+6+R0xKimdxr+mS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zUcm3vp6jRZL"},"outputs":[],"source":["\"\"\"Seminar 6. Image Binary Classification with Keras. ML ops.\"\"\"\n","import argparse\n","import os\n","import zipfile\n","import shutil\n","from urllib.request import urlretrieve\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow as tf\n","import boto3 # для S3\n","import dotenv\n","\n","GOOGLE_COLAB_LINK = \"https://colab.research.google.com/drive/1b4-VaznVIqjgIhjMNZFpupBgPbKMiRGn#scrollTo=uXp-HdR3Xewe\"\n","DATA_URL = 'https://storage.yandexcloud.net/fa-bucket/cats_dogs_train.zip'\n","PATH_TO_DATA_ZIP = 'data/raw/cats_dogs_train.zip'\n","PATH_TO_DATA = 'data/raw/cats_dogs_train'\n","PATH_TO_MODEL = 'models/model_6'\n","BUCKET_NAME = 'neuralnets2023'\n","YOUR_GIT_USER = 'meribabayaan'\n","image_size = (180, 180)\n","batch_size = 64\n","\n","def download_data():\n","    \"\"\"Pipeline: download and extract data\"\"\"\n","    if not os.path.exists(PATH_TO_DATA_ZIP):\n","        print('Downloading data...')\n","        urlretrieve(DATA_URL, PATH_TO_DATA_ZIP)\n","    else:\n","        print('Data is already downloaded!')\n","\n","    if not os.path.exists(PATH_TO_DATA):\n","        print('Extracting data...')\n","        with zipfile.ZipFile(PATH_TO_DATA_ZIP, 'r') as zip_ref:\n","            zip_ref.extractall(PATH_TO_DATA)\n","    else:\n","        print('Data is already extracted!')\n","\n","\n","def filter_and_augment_data():\n","    num_skipped = 0\n","    path_to_images = os.path.join(PATH_TO_DATA, \"PetImages\")\n","    for folder_name in (\"Cat\", \"Dog\"):\n","        folder_path = os.path.join(path_to_images, folder_name)\n","        for fname in os.listdir(folder_path):\n","            fpath = os.path.join(folder_path, fname)\n","            try:\n","                fobj = open(fpath, \"rb\")\n","                is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n","            finally:\n","                fobj.close()\n","\n","            if not is_jfif:\n","                num_skipped += 1\n","                # Delete corrupted image\n","                os.remove(fpath)\n","\n","    print(\"Deleted %d images\" % num_skipped)\n","\n","    train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n","        path_to_images,\n","        validation_split=0.2,\n","        subset=\"both\",\n","        seed=1337,\n","        image_size=image_size,\n","        batch_size=batch_size,\n","    )\n","\n","    data_augmentation = keras.Sequential(\n","        [\n","            layers.RandomFlip(\"horizontal\"),\n","            layers.RandomRotation(0.1),\n","        ]\n","    )\n","    train_ds = train_ds.map(\n","        lambda img, label: (data_augmentation(img), label),\n","        num_parallel_calls=tf.data.AUTOTUNE,\n","    )\n","\n","    # Prefetching samples in GPU memory helps maximize GPU utilization.\n","    train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n","    val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n","\n","    return train_ds, val_ds\n","\n","\n","def make_model(input_shape, num_classes):\n","    inputs = keras.Input(shape=input_shape)\n","\n","    x = layers.Rescaling(1.0 / 255)(inputs)\n","    x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    previous_block_activation = x  # Set aside residual\n","\n","    for size in [256, 512, 728]:\n","        x = layers.Activation(\"relu\")(x)\n","        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.Activation(\"relu\")(x)\n","        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n","\n","        # Project residual\n","        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n","            previous_block_activation\n","        )\n","        x = layers.add([x, residual])  # Add back residual\n","        previous_block_activation = x  # Set aside next residual\n","\n","    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.GlobalAveragePooling2D()(x)\n","    if num_classes == 2:\n","        activation = \"sigmoid\"\n","        units = 1\n","    else:\n","        activation = \"softmax\"\n","        units = num_classes\n","\n","    x = layers.Dropout(0.5)(x)\n","    outputs = layers.Dense(units, activation=activation)(x)\n","    return keras.Model(inputs, outputs)\n","\n","\n","def train():\n","    \"\"\"Pipeline: Build, train and save model to models/model_6\"\"\"\n","    print('Training model')\n","    train_ds, val_ds = filter_and_augment_data()\n","    model = make_model(input_shape=[*image_size, 3], num_classes=2)\n","    epochs = 7\n","\n","    callbacks = [\n","        tf.keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\"),\n","    ]\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(1e-3),\n","        loss=\"binary_crossentropy\",\n","        metrics=[\"accuracy\"],\n","    )\n","    model.fit(\n","        train_ds,\n","        epochs=epochs,\n","        callbacks=callbacks,\n","        validation_data=val_ds,\n","    )\n","    model.save(PATH_TO_MODEL)\n","\n","\n","def upload():\n","    \"\"\"Pipeline: Upload model to S3 storage\"\"\"\n","    print('Upload model')\n","    zip_model_path = PATH_TO_MODEL+'.zip'\n","    shutil.make_archive(base_name=PATH_TO_MODEL,\n","                        format='zip',\n","                        root_dir=PATH_TO_MODEL)\n","\n","    config = dotenv.dotenv_values(\"env\")\n","\n","    ACCESS_KEY = config['ACCESS_KEY']\n","    SECRET_KEY = config['SECRET_KEY']\n","\n","    client = boto3.client(\n","        's3',\n","        endpoint_url='https://storage.yandexcloud.net',\n","        aws_access_key_id=ACCESS_KEY,\n","        aws_secret_access_key=SECRET_KEY\n","    )\n","\n","    client.upload_file(zip_model_path, BUCKET_NAME, f'{YOUR_GIT_USER}/model_6.zip')\n","\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(\n","        prog='src/seminar6.py',\n","        description='Typical DL lifecycle pipelines.')\n","    parser.add_argument('--download', action='store_true', help='Download images and extract to data/raw directory')\n","    parser.add_argument('--train', action='store_true', help='Build, train and save model to models/seminar6_model')\n","    parser.add_argument('--upload', action='store_true', help='Upload model to S3 storage')\n","    args = parser.parse_args()\n","    if args.download:\n","        download_data()\n","    if args.train:\n","        train()\n","    if args.upload:\n","        upload()"]}]}